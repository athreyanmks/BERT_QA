# BERT_QA
## Finetuning BERT LLM on SQuAD(Stanford Question Answering Dataset)

The objective is to fine tune the bert-base-uncased model and compare the performance to the distilbert-base-cased-distilled-squad pretuned model.

### Finetuned model Performance
<img src="results/b0f0875e-c834-4fa7-8932-c5a8e62cf1a3.png" width="300" />

### Pretuned model Performance
<img src="results/bfde37a2-d600-4c1c-8481-5be142b8d449.png" width="300" />

Further comparisons between the case by case performance, techniques to improve the fine tuned model and further actions that can be taken are mentioned in the BERT_QA.pdf 
